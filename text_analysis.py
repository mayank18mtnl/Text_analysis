# -*- coding: utf-8 -*-
"""Text_analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1iGOaIEEkguDI7nOJjHduonlwOnaREqew
"""

pip install requests beautifulsoup4

pip install textblob

pip install Textstat

import nltk
from nltk.corpus import stopwords
from bs4 import BeautifulSoup
from nltk.sentiment import SentimentIntensityAnalyzer
import requests
from textblob import TextBlob
from nltk.tokenize import sent_tokenize, word_tokenize
from textstat import syllable_count, textstat
from nltk.tag import pos_tag
import pyphen
nltk.download('punkt')
nltk.download('vader_lexicon')
nltk.download('stopwords')
nltk.download('averaged_perceptron_tagger')

import requests
from bs4 import BeautifulSoup
import nltk
from nltk.tokenize import word_tokenize, sent_tokenize
from nltk.corpus import stopwords
from nltk.tag import pos_tag
from textblob import TextBlob
import pyphen
import pandas as pd


def analyze_text_from_url(url_id,url):
    def extract_text_from_url(url):                                             # Function to extract text from a given URL
        response = requests.get(url)
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, 'html.parser')
            return soup.get_text()
        else:
            print(f"Error: Unable to fetch content from the URL. Status code: {response.status_code}")
            return None

    def remove_stop_words(text):                                                # Function to remove stop words from the text
        words = nltk.word_tokenize(text)
        stop_words = set(stopwords.words('english'))
        filtered_words = [word for word in words if word.lower() not in stop_words]
        return ' '.join(filtered_words)

    def count_syllables(word):                                                  # Function to count syllables in a word using Pyphen
        return len(dic.inserted(word).split('-'))

    extracted_text = extract_text_from_url(url)                                 # Extract text from the URL

    if extracted_text:
        text_without_stopwords = remove_stop_words(extracted_text)              # Remove stop words from the extracted text

        blob = TextBlob(text_without_stopwords)                                 # Create a TextBlob object

        sentiment_score = blob.sentiment.polarity                               # Sentiment analysis
        positive_score = max(sentiment_score, 0)
        negative_score = -min(sentiment_score, 0)

        sentences = sent_tokenize(text_without_stopwords)                       # Tokenize the text into sentences and words
        words = nltk.word_tokenize(text_without_stopwords)

        # Calculate various metrics
        polarity_score = blob.sentiment.polarity
        subjectivity_score = blob.sentiment.subjectivity
        average_sentence_length = sum(len(word_tokenize(sentence)) for sentence in sentences) / len(sentences)
        count_multi_syllable_words = sum(count_syllables(word) > 1 for word in words)
        count_complex_words = sum(count_syllables(word) > 2 for word in words)
        fog_index = 0.4 * (average_sentence_length + (count_complex_words / len(sentences)) * 100)
        word_numbers = len(words)
        per_complex_words = (count_complex_words / word_numbers) * 100
        word_counts_in_sentences = [len(word_tokenize(sentence)) for sentence in sentences]
        average_words_per_sentence = sum(word_counts_in_sentences) / len(sentences)
        pos_tags = pos_tag(words)
        personal_pronoun_tags = ['I', 'we', 'my', 'ours', 'us']
        personal_pronouns_count = sum(1 for word, pos_tag in pos_tags if pos_tag in personal_pronoun_tags)
        total_characters = sum(len(word) for word in words)
        average_word_length = total_characters / len(words)

        syllables_per_word = [count_syllables(word) for word in words]          # Calculate syllables per word using Pyphen
        average_syllables_per_word = sum(syllables_per_word) / len(words)

        # Print the results
        print(f"Positive Score: {positive_score}")
        print(f"Negative Score: {negative_score}")
        print(f"Polarity Score: {polarity_score}")
        print(f"Subjectivity Score: {subjectivity_score}")
        print(f"Average Sentence Length: {average_sentence_length} words")
        print(f"Percentage of complex words: {per_complex_words}")
        print(f"Fog Index: {fog_index:.2f}")
        print(f"Average Number of Words Per Sentence: {average_words_per_sentence:.2f}")
        print(f"Number of complex words: {count_multi_syllable_words}")
        print(f"Number of Words: {word_numbers}")
        print(f"Average Syllables Per Word: {average_syllables_per_word:.2f}")
        print(f"Number of Personal Pronouns: {personal_pronouns_count}")
        print(f"Average Word Length: {average_word_length:.2f}")

        analysis_data = {
            'URL_ID':[url_id],
            'URL': [url],
            'Positive Score': [positive_score],
            'Negative Score': [negative_score],
            'Polarity Score': [polarity_score],
            'Subjectivity Score': [subjectivity_score],
            'Average Sentence Length': [average_sentence_length],
            'Percentage of Complex Words': [per_complex_words],
            'Fog Index': [fog_index],
            'Average Number of Words Per Sentence': [average_words_per_sentence],
            'Number of Complex Words': [count_multi_syllable_words],
            'Number of Words': [word_numbers],
            'Syllables Per Word': [average_syllables_per_word],
            'Number of Personal Pronouns': [personal_pronouns_count],
            'Average Word Length': [average_word_length]
        }

        excel_file='/content/drive/MyDrive/intership/Output_bc.xlsx'
        try:                                                                    # Load the existing DataFrame or create a new one
            df = pd.read_excel(excel_file)
        except FileNotFoundError:
            df = pd.DataFrame()

        df = df.append(pd.DataFrame(analysis_data), ignore_index=True)          # Append the analysis data to the DataFrame
        df.to_excel(excel_file, index=False)                                     # Write the updated DataFrame back to the Excel file

url=input("Enter the url : ")
url_id=input("Enter the URL ID : ")
analyze_text_from_url(url_id,url)
df = pd.read_excel('/content/drive/MyDrive/intership/Output_bc.xlsx')
df



11668
17671.4